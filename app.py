import os
import logging
import random
import asyncio
from flask import Flask, request, jsonify
from telegram import Update
from telegram.ext import (
    Application,
    CommandHandler,
    MessageHandler,
    filters,
    ContextTypes
)
from dotenv import load_dotenv
import ollama

# Configura√ß√µes iniciais
load_dotenv()
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)

app = Flask(__name__)
TELEGRAM_TOKEN = os.getenv('TELEGRAM_TOKEN')

# Dados est√°ticos da FURIA
FURIA_DATA = {
    "elenco": {
        "jogadores": [
            {"nome": "KSCERATO", "funcao": "Entry Fragger", "rating": 1.24},
            {"nome": "yuurih", "funcao": "Rifler", "rating": 1.18},
            {"nome": "arT", "funcao": "IGL/Capit√£o", "rating": 1.10},
            {"nome": "saffee", "funcao": "AWPer", "rating": 1.15},
            {"nome": "chelo", "funcao": "Support", "rating": 1.08}
        ],
        "tecnico": "guerri"
    },
    "proximos_jogos": [
        {"adversario": "Team Vitality", "data": "25/05", "horario": "19:00", "torneio": "ESL Pro League"},
        {"adversario": "Natus Vincere", "data": "01/06", "horario": "16:00", "torneio": "BLAST Premier"}
    ]
}

RESPOSTAS = {
    "start": "üêÜüî• *FURIA FanBot* üî•üêÜ\n\nPergunte sobre jogadores, pr√≥ximos jogos ou resultados!",
    "elenco": "üë• *Elenco FURIA* üë•\n" + "\n".join(
        f"‚û° {p['nome']} ({p['funcao']}) | Rating: {p['rating']}" 
        for p in FURIA_DATA["elenco"]["jogadores"]
    ),
    "proximos_jogos": "üóìÔ∏è *Pr√≥ximos Jogos*\n" + "\n".join(
        f"‚öîÔ∏è vs {j['adversario']} ({j['torneio']})\n‚è∞ {j['data']} √†s {j['horario']} BRT" 
        for j in FURIA_DATA["proximos_jogos"]
    ),
    "motivacional": [
        "üéâ VAMO JUNT√ÉO! #DIADEFURIA",
        "üî• ESSE ANO √â NOSSO!",
        "üêÜ FURIA √â GUERREIRA!"
    ]
}

def generate_ai_response(prompt: str) -> str:
    """Gera resposta usando Ollama com contexto da FURIA (vers√£o s√≠ncrona)"""
    try:
        response = ollama.chat(
            model='llama3.2:latest',  # Altere para o modelo que aparece no seu 'ollama list'
            messages=[
                {
                    "role": "system",
                    "content": f"""
                    Voc√™ √© o FURIA FanBot, um assistente virtual que responde como torcedor animado.
                    Dados atuais: {FURIA_DATA}
                    Regras:
                    - Responda em portugu√™s brasileiro
                    - Seja breve e animado
                    - Use emojis quando apropriado
                    - Mantenha o foco em CS:GO/FURIA/VALORANT/LEAGUE OF LEGENDS
                    
                    """
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            options={'temperature': 0.7, 'num_predict': 100}
        )
        return response['message']['content']
    except Exception as e:
        logging.error(f"Erro Ollama: {e}")
        return None

async def get_static_response(prompt: str) -> str:
    """Fallback com respostas est√°ticas"""
    prompt_lower = prompt.lower()
    
    if any(p["nome"].lower() in prompt_lower for p in FURIA_DATA["elenco"]["jogadores"]):
        return RESPOSTAS["elenco"]
    elif any(t in prompt_lower for t in ["jogo", "partida", "calend√°rio"]):
        return RESPOSTAS["proximos_jogos"]
    elif any(t in prompt_lower for t in ["vamo", "bora", "furia"]):
        return random.choice(RESPOSTAS["motivacional"])
    else:
        return "ü§ñ Pergunte sobre: /jogadores, /proximojogos"

# Handlers do Telegram
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    await update.message.reply_text(RESPOSTAS["start"], parse_mode="Markdown")

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    user_input = update.message.text
    
    # Chamada ass√≠ncrona da fun√ß√£o s√≠ncrona
    loop = asyncio.get_event_loop()
    ai_response = await loop.run_in_executor(None, generate_ai_response, user_input)
    
    if ai_response:
        # Limita a resposta para evitar erros no Telegram
        await update.message.reply_text(ai_response[:400], parse_mode="Markdown")
        return
    
    # Fallback est√°tico
    static_response = await get_static_response(user_input)
    await update.message.reply_text(static_response, parse_mode="Markdown")

def setup_bot():
    application = Application.builder().token(TELEGRAM_TOKEN).build()
    
    # Comandos
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("jogadores", 
        lambda u,c: u.message.reply_text(RESPOSTAS["elenco"], parse_mode="Markdown")))
    application.add_handler(CommandHandler("proximojogos", 
        lambda u,c: u.message.reply_text(RESPOSTAS["proximos_jogos"], parse_mode="Markdown")))
    
    # Mensagens gen√©ricas
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    return application

# Rotas Flask
@app.route('/webhook', methods=['POST'])
async def webhook():
    application = setup_bot()
    update = Update.de_json(request.get_json(), application.bot)
    await application.process_update(update)
    return jsonify(status="success")

@app.route('/')
def health_check():
    return "üêÜ FURIA FanBot Online - Use /start no Telegram"

if __name__ == '__main__':
    # Verifica conex√£o com Ollama
    try:
        models = ollama.list()
        logging.info(f"Modelos Ollama dispon√≠veis: {[m['name'] for m in models['models']]}")
        logging.info("Bot pronto para receber mensagens!")
    except Exception as e:
        logging.error(f"Falha ao conectar no Ollama: {e}")
        logging.warning("O bot usar√° apenas respostas est√°ticas")

    bot = setup_bot()
    bot.run_polling()